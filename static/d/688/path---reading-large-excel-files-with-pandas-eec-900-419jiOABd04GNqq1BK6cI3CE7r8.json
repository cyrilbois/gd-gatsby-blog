{"data":{"site":{"siteMetadata":{"title":"Giacomo Debidda","author":"Giacomo Debidda","description":"Giacomo Debidda's Personal Blog"}},"markdownRemark":{"id":"ce9d01bd-98f7-53cf-81a8-fe24b8a684cb","excerpt":"Last week I took part in a  Dataviz Battle on the dataisbeautiful subreddit , where we had to create a visualization from the  TSA claims…","timeToRead":3,"html":"<p>Last week I took part in a <a href=\"https://www.reddit.com/r/dataisbeautiful/comments/950j3n/battle_dataviz_battle_for_the_month_of_august/\">Dataviz Battle on the dataisbeautiful subreddit</a>, where we had to create a visualization from the <a href=\"https://www.dhs.gov/tsa-claims-data\">TSA claims dataset</a>. I like these kind of competitions because most of the time you end up learning a lot of useful things along the way.</p>\n<p>This time the data was quite clean, but it was scattered across several PDF files and Excel files. In the process of extracting data from PDFs I got to know some tools and libraries, and in the end I used <a href=\"https://github.com/chezou/tabula-py\">tabula-py</a>, a Python wrapper for the Java library <a href=\"https://tabula.technology/\">tabula</a>. As for the Excel files, I found out that a one-liner - a simple <code class=\"language-text\">pd.read_excel</code> - wasn’t enough.</p>\n<p>The biggest Excel file was ~7MB and contained a single worksheet with ~100k lines. I though Pandas could read the file in one go without any issue (I have 10GB of RAM on my computer), but apparently I was wrong.</p>\n<p>The solution was to read the file in chunks. The <code class=\"language-text\">pd.read_excel</code> function doesn’t have a cursor like <code class=\"language-text\">pd.read_sql</code>, so I had to implement this logic manually. Here is what I did:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> os\n<span class=\"token keyword\">import</span> pandas <span class=\"token keyword\">as</span> pd\n\n\nHERE <span class=\"token operator\">=</span> os<span class=\"token punctuation\">.</span>path<span class=\"token punctuation\">.</span>abspath<span class=\"token punctuation\">(</span>os<span class=\"token punctuation\">.</span>path<span class=\"token punctuation\">.</span>dirname<span class=\"token punctuation\">(</span>__file__<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nDATA_DIR <span class=\"token operator\">=</span> os<span class=\"token punctuation\">.</span>path<span class=\"token punctuation\">.</span>abspath<span class=\"token punctuation\">(</span>os<span class=\"token punctuation\">.</span>path<span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span>HERE<span class=\"token punctuation\">,</span> <span class=\"token string\">'..'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'data'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">make_df_from_excel</span><span class=\"token punctuation\">(</span>file_name<span class=\"token punctuation\">,</span> nrows<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token triple-quoted-string string\">\"\"\"Read from an Excel file in chunks and make a single DataFrame.\n\n    Parameters\n    ----------\n    file_name : str\n    nrows : int\n        Number of rows to read at a time. These Excel files are too big, so we\n        can't read all rows in one go.\n    \"\"\"</span>\n    file_path <span class=\"token operator\">=</span> os<span class=\"token punctuation\">.</span>path<span class=\"token punctuation\">.</span>abspath<span class=\"token punctuation\">(</span>os<span class=\"token punctuation\">.</span>path<span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span>DATA_DIR<span class=\"token punctuation\">,</span> file_name<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    xl <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>ExcelFile<span class=\"token punctuation\">(</span>file_path<span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\"># In this case, there was only a single Worksheet in the Workbook.</span>\n    sheetname <span class=\"token operator\">=</span> xl<span class=\"token punctuation\">.</span>sheet_names<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span>\n\n    <span class=\"token comment\"># Read the header outside of the loop, so all chunk reads are consistent </span>\n    <span class=\"token comment\"># across all loop iterations.</span>\n    df_header <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>read_excel<span class=\"token punctuation\">(</span>file_path<span class=\"token punctuation\">,</span> sheetname<span class=\"token operator\">=</span>sheetname<span class=\"token punctuation\">,</span> nrows<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>f<span class=\"token string\">\"Excel file: {file_name} (worksheet: {sheetname})\"</span><span class=\"token punctuation\">)</span>\n    \n    chunks <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n    i_chunk <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n    <span class=\"token comment\"># The first row is the header. We have already read it, so we skip it now.</span>\n    skiprows <span class=\"token operator\">=</span> <span class=\"token number\">1</span>\n    <span class=\"token keyword\">while</span> <span class=\"token boolean\">True</span><span class=\"token punctuation\">:</span>\n        df_chunk <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>read_excel<span class=\"token punctuation\">(</span>\n            file_path<span class=\"token punctuation\">,</span> sheetname<span class=\"token operator\">=</span>sheetname<span class=\"token punctuation\">,</span>\n            nrows<span class=\"token operator\">=</span>nrows<span class=\"token punctuation\">,</span> skiprows<span class=\"token operator\">=</span>skiprows<span class=\"token punctuation\">,</span> header<span class=\"token operator\">=</span><span class=\"token boolean\">None</span><span class=\"token punctuation\">)</span>\n        skiprows <span class=\"token operator\">+=</span> nrows\n        <span class=\"token comment\"># When there is no data, we know we can break out of the infinite loop.</span>\n        <span class=\"token keyword\">if</span> <span class=\"token operator\">not</span> df_chunk<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">break</span>\n        <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>f<span class=\"token string\">\"  - chunk {i_chunk} ({df_chunk.shape[0]} rows)\"</span><span class=\"token punctuation\">)</span>\n            chunks<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>df_chunk<span class=\"token punctuation\">)</span>\n        i_chunk <span class=\"token operator\">+=</span> <span class=\"token number\">1</span>\n    \n    df_chunks <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>concat<span class=\"token punctuation\">(</span>chunks<span class=\"token punctuation\">)</span>\n    <span class=\"token comment\"># We need to rename the columns to concatenate the chunks with the header.</span>\n    columns <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span>i<span class=\"token punctuation\">:</span> col <span class=\"token keyword\">for</span> i<span class=\"token punctuation\">,</span> col <span class=\"token keyword\">in</span> <span class=\"token builtin\">enumerate</span><span class=\"token punctuation\">(</span>df_header<span class=\"token punctuation\">.</span>columns<span class=\"token punctuation\">.</span>tolist<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">}</span>    \n    df_chunks<span class=\"token punctuation\">.</span>rename<span class=\"token punctuation\">(</span>columns<span class=\"token operator\">=</span>columns<span class=\"token punctuation\">,</span> inplace<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n    df <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>concat<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>df_header<span class=\"token punctuation\">,</span> df_chunks<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> df\n\n\n<span class=\"token keyword\">if</span> __name__ <span class=\"token operator\">==</span> <span class=\"token string\">'__main__'</span><span class=\"token punctuation\">:</span>\n    df <span class=\"token operator\">=</span> make_df_from_excel<span class=\"token punctuation\">(</span><span class=\"token string\">'claims-2002-2006_0.xls'</span><span class=\"token punctuation\">,</span> nrows<span class=\"token operator\">=</span><span class=\"token number\">10000</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>Another thing to keep in mind. When working with <a href=\"http://www.python-excel.org/\">Excel files in Python</a>, you might need to use different packages whether you need to read/write data from/to <code class=\"language-text\">.xls</code> and <code class=\"language-text\">.xlsx</code> files.</p>\n<p>This dataset contained both <code class=\"language-text\">.xls</code> and <code class=\"language-text\">.xlsx</code> files, so I had to use <a href=\"https://github.com/python-excel/xlrd\">xlrd</a> to read them. Please <a href=\"https://groups.google.com/forum/#!msg/python-excel/P6TjJgFVjMI/g8d0eWxTBQAJ\">be aware</a> that if your only concern is reading <code class=\"language-text\">.xlsx</code> files, then openpyxl is the way to go, even if xlrd <a href=\"https://stackoverflow.com/questions/35823835/reading-excel-file-is-magnitudes-slower-using-openpyxl-compared-to-xlrd\">could still be faster</a>.</p>\n<p>This time I didn’t have to write any Excel files, but if you need to, then you want <a href=\"https://xlsxwriter.readthedocs.io/\">xlsxwriter</a>. I remember having used it to create workbooks (i.e. Excel files) with many complex worksheets and cell comments. It’s really an amazing tool!</p>","frontmatter":{"title":"Reading large Excel files with Pandas","date":"September 01, 2018","tags":["Python","Pandas"]}}},"pageContext":{"slug":"/reading-large-excel-files-with-pandas/","previous":{"fields":{"slug":"/visualize-earthquakes-with-plotly-dash/"},"timeToRead":9,"frontmatter":{"title":"Visualize Earthquakes with Plotly Dash","path":"/visualize-earthquakes-with-plotly-dash/"}},"next":{"fields":{"slug":"/a-simple-git-hook-for-your-python-projects/"},"timeToRead":1,"frontmatter":{"title":"A simple git hook for your Python projects","path":"/a-simple-git-hook-for-your-python-projects/"}}}}